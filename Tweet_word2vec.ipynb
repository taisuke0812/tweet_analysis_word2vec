{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/taisuke0812/tweet_analysis_word2vec/blob/master/Tweet_word2vec.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "etxCmdfbGD0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   必要なライブラリをインストールする\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mWRSePB7F-gl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install janome\n",
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4YhGuQQyGI_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ツイートをリストに変換する"
      ]
    },
    {
      "metadata": {
        "id": "ZaFrEbq5BbwY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import glob\n",
        "from janome.tokenizer import Tokenizer\n",
        "from gensim.models import word2vec\n",
        "\n",
        "def tokenize(text):\n",
        "    t = Tokenizer()\n",
        "    tokens = t.tokenize(\",\".join(text))\n",
        "    word = []\n",
        "    for token in tokens:\n",
        "        part_of_speech = token.part_of_speech.split(\",\")[0]\n",
        " \n",
        "        if part_of_speech == \"名詞\":\n",
        "            word.append(token.surface)        \n",
        "        #if part_of_speech == \"動詞\":        \n",
        "            #word.append(token.surface)\n",
        "        if part_of_speech == \"形容詞\":\n",
        "            word.append(token.surface)        \n",
        "        if part_of_speech == \"形容動詞\":        \n",
        "            word.append(token.surface)            \n",
        "    return word\n",
        "\n",
        "\n",
        "tweets = pd.read_csv(\"tweets.csv\",engine = \"python\")\n",
        "\n",
        "tweets = tweets.drop(\"tweet_id\",axis=1)\n",
        "tweets = tweets.drop(\"timestamp\",axis=1)\n",
        "tweets = tweets.drop(\"source\",axis=1)\n",
        "tweets = tweets.drop(\"retweeted_status_id\",axis=1)\n",
        "tweets = tweets.drop(\"retweeted_status_user_id\",axis=1)\n",
        "tweets = tweets.drop(\"expanded_urls\",axis=1)\n",
        "tweets = tweets.drop(\"retweeted_status_timestamp\",axis=1)\n",
        "#tweets.head(20)\n",
        "\n",
        "tweets[\"in_reply_to_status_id\"].fillna(\"No_data\",inplace=True)\n",
        "\n",
        "new_tweet = []\n",
        "\n",
        "#誰かへのリプを削除\n",
        "for i in range(len(tweets)):\n",
        "   if(tweets[\"in_reply_to_status_id\"][i] == \"No_data\"):\n",
        "      new_tweet.append(tweets[\"text\"][i])\n",
        "      \n",
        "      \n",
        "#RTを削除\n",
        "data = []\n",
        "pattern = r\"RT\"\n",
        "for j in range(len(new_tweet)):\n",
        "  if(not(re.match(pattern,new_tweet[j]))):\n",
        "      data.append(new_tweet[j])\n",
        "\n",
        "#リンクを含むツイートを削除\n",
        "data_ = []\n",
        "patterns = r\"https\"\n",
        "for k in range(len(data)):\n",
        "  if(not(re.search(patterns,data[k]))):\n",
        "    data_.append(data[k])\n",
        "      \n",
        "#リプが消せ切れてなかったので\n",
        "data__ = []\n",
        "pattern_ = r\"@\"\n",
        "for l in range(len(data_)):\n",
        "    if(not(re.search(pattern_,data_[l]))):\n",
        "        data__.append(data_[l])\n",
        "\n",
        "\n",
        "\n",
        "tweet_data = pd.DataFrame(data__)\n",
        "\n",
        "tweet_data.head(120)\n",
        "\n",
        "tweets = np.array(tweet_data)\n",
        "word = \"\"\n",
        "for text in tweets:\n",
        "  word += text  \n",
        "text = tokenize(word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RAHlZ2ViGOYJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "学習させて、結果を調べる\n"
      ]
    },
    {
      "metadata": {
        "id": "HuRWQipsB3ku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "116151ba-3d73-461c-b640-5d25c75cec04"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model = word2vec.Word2Vec([text], size=400, min_count=20, window=15)\n",
        "print(model.most_similar(positive=[\"高専\"]))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ん', 0.9999973177909851), ('こと', 0.9999971389770508), ('の', 0.9999970197677612), ('人', 0.9999968409538269), ('マジ', 0.999996542930603), ('いい', 0.999996542930603), ('学校', 0.9999963045120239), ('ない', 0.9999962449073792), ('笑', 0.9999961256980896), ('...', 0.9999960064888)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "_pw9ZlKOGcDi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kOdPLpdlCtaY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "j8rxhjUBBWRz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "6vvc9lbfMHdZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sfzwQeOxNB3k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}